version: '3.8'

services:
  research-agent:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: deep-learning-research-agent
    environment:
      # API Keys - Set these in .env.docker or override in docker-compose.override.yml
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - TAVILY_API_KEY=${TAVILY_API_KEY}
      - MODEL_PROVIDER=${MODEL_PROVIDER}
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL}
      - OLLAMA_MODEL=${OLLAMA_MODEL}
    volumes:
      # Mount source code for development (comment out for production)
      - .:/app
      # Mount .env file if it exists
      - ./.env:/app/.env:ro
    networks:
      - research-network
    # Uncomment to run interactively
    # stdin_open: true
    # tty: true
    # Override default command for interactive use
    # command: ["uv", "run", "python", "main.py", "--query", "Your research question here"]

  # Optional: Add Ollama service for local model usage
  # ollama:
  #   image: ollama/ollama:latest
  #   container_name: ollama
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ollama_data:/root/.ollama
  #   networks:
  #     - research-network
  #   # Uncomment to automatically pull a model
  #   # environment:
  #   #   - OLLAMA_MODELS=deepseek-r1:32b

networks:
  research-network:
    driver: bridge

# volumes:
#   ollama_data:
