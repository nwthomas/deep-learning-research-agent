# Set values here for LangSmith evaluation and tracing
LANGSMITH_API_KEY=your_langsmith_api_key_here
LANGSMITH_TRACING=true
LANGSMITH_PROJECT=deep-learning-research-agent

# First, set what the model provider is. If a local model is being used, set it to "local". Otherwise,
# check out what Langchain allows for the model_provider argument here (under the parameters section
# for model_provider):
# https://python.langchain.com/api_reference/langchain/chat_models/langchain.chat_models.base.init_chat_model.html
MODEL_PROVIDER=your_model_provider_here

# If using Anthropic, set the API key here
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# If using local models, set the Ollama server hostname/ip address + port here
OLLAMA_BASE_URL=your_ollama_hostname_here

# Set the model to use here (either for Anthropic or Ollama)
MODEL_NAME=your_model_name_here

# Set a Tavily key here to allow internet searches from tool calls
TAVILY_API_KEY=your_tavily_api_key_here

